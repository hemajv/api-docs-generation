{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4a592f-7f35-4f1a-b762-a63f9b0a979f",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef4e1a0-6389-450c-a601-3386938c97ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from genai import Credentials, Client\n",
    "from genai.text.generation import TextGenerationParameters\n",
    "from genai.text.tokenization import (\n",
    "    TextTokenizationParameters,\n",
    "    TextTokenizationReturnOptions,\n",
    "    TextTokenizationCreateResults,\n",
    ")\n",
    "from genai.credentials import Credentials\n",
    "import sys\n",
    "sys.path.append('../../app')\n",
    "from utils import eval_using_model, generate_text_using_OpenAI, generate_prompt, generate_text\n",
    "from langchain.evaluation import (\n",
    "    Criteria,\n",
    "    load_evaluator,\n",
    "    EvaluatorType\n",
    ")\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913380ae-b833-4b47-a286-0d02b87a983f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(model_id, api_key, openai_key, file, instruction, functions, classes, documentation, imports, other, functions_code, functions_doc, classes_code, classes_doc):\n",
    "\n",
    "    \n",
    "    DATASET_PATH = \"../../data/raw/chunked_data.json\"\n",
    "\n",
    "    with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    code = data[file][\"code_chunks\"]\n",
    "\n",
    "    actual_doc = data[file][\"markdown\"]\n",
    "\n",
    "    functions_text = code[\"functions\"]\n",
    "    classes_text = code[\"classes\"]\n",
    "    documentation_text = code[\"documentation\"]\n",
    "    imports_text = code[\"imports\"]\n",
    "    other_text = code[\"other\"]\n",
    "    functions_code_text = code[\"functions_code\"]\n",
    "    functions_doc_text = code[\"functions_docstrings\"]\n",
    "    classes_code_text = code[\"classes_code\"]\n",
    "    classes_doc_text = code[\"classes_docstrings\"]\n",
    "\n",
    "\n",
    "    prompt = generate_prompt(\n",
    "        instruction=instruction,\n",
    "        functions=functions,\n",
    "        functions_text=functions_text,\n",
    "        classes=classes,\n",
    "        classes_text=classes_text,\n",
    "        documentation=documentation,\n",
    "        documentation_text=documentation_text,\n",
    "        imports=imports,\n",
    "        imports_text=imports_text,\n",
    "        other=other,\n",
    "        other_text=other_text,\n",
    "        functions_code=functions_code,\n",
    "        functions_code_text=functions_code_text,\n",
    "        functions_doc=functions_doc,\n",
    "        functions_doc_text=functions_doc_text,\n",
    "        classes_code=classes_code,\n",
    "        classes_code_text=classes_code_text,\n",
    "        classes_doc=classes_doc,\n",
    "        classes_doc_text=classes_doc_text,\n",
    "    )\n",
    "\n",
    "    if model_id == \"OpenAI/gpt3.5\":\n",
    "        result = generate_text_using_OpenAI(prompt, openai_key)\n",
    "\n",
    "    else:\n",
    "        result = generate_text(model_id, prompt, decoding_method=\"sample\", max_new_tokens=1024, temperature=0.7, top_k=50, top_p=0.50, genai_key=api_key)\n",
    "    \n",
    "    return prompt, result, actual_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621485af-2dd9-4006-bed0-be6f539ac0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_scores(gpt_score):\n",
    "    pattern = r'(\\w+):\\s(\\d+)'\n",
    "    matches = re.findall(pattern, gpt_score)\n",
    "\n",
    "    evaluation_scores = {match[0]: int(match[1]) for match in matches}\n",
    "\n",
    "    gpt_accuracy_score = evaluation_scores['Accuracy']\n",
    "    gpt_relevance_score = evaluation_scores['Relevance']\n",
    "    gpt_clarity_score = evaluation_scores['Clarity']\n",
    "    gpt_completeness_score = evaluation_scores['Completeness']\n",
    "    gpt_readability_score = evaluation_scores['Readability']\n",
    "    \n",
    "    return gpt_accuracy_score, gpt_relevance_score, gpt_clarity_score, gpt_completeness_score, gpt_readability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e447297-4fff-4916-bd99-ffeb522f3a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def langchain_scores(generated_patch, prompt, actual_doc):\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    evaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"helpfulness\")\n",
    "    eval_result = evaluator.evaluate_strings(prediction=generated_patch, input=prompt)\n",
    "    print(eval_result)\n",
    "    langchain_helpfulness = eval_result['score']\n",
    "    \n",
    "    evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria=\"correctness\")\n",
    "    eval_result = evaluator.evaluate_strings(prediction=generated_patch, input=prompt, reference=actual_doc)\n",
    "    print(eval_result)\n",
    "    langchain_correctness = eval_result['score']\n",
    "\n",
    "    custom_criteria = {\n",
    "    \"logical\": \"Is the output logical and complete? Does it capture all required fields\"\n",
    "                    }\n",
    "    eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    "    llm=llm\n",
    "    )\n",
    "    eval_result = eval_chain.evaluate_strings(prediction=generated_patch, input=prompt)\n",
    "    print(eval_result)\n",
    "    langchain_logical = eval_result['score']\n",
    "    \n",
    "    return langchain_helpfulness, langchain_correctness, langchain_logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58ae507-78a0-4eb8-990e-7f4af20d597e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_row_to_dataframe(df, prompt, generated_patch, gpt_accuracy_score, gpt_relevance_score, gpt_clarity_score, gpt_completeness_score, gpt_readability_score, langchain_helpfulness, langchain_correctness, langchain_logical):\n",
    "\n",
    "    new_row = {\n",
    "        'prompt': prompt,\n",
    "        'response': generated_patch,\n",
    "        'gpt_accuracy_score': gpt_accuracy_score,\n",
    "        'gpt_relevance_score': gpt_relevance_score,\n",
    "        'gpt_clarity_score' : gpt_clarity_score,\n",
    "        'gpt_completeness_score' : gpt_completeness_score,\n",
    "        'gpt_readability_score' : gpt_readability_score,\n",
    "        'langchain_helpfulness' : langchain_helpfulness,\n",
    "        'langchain_correctness' : langchain_correctness,\n",
    "        'langchain_logical' : langchain_logical\n",
    "    }\n",
    "\n",
    "    df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
